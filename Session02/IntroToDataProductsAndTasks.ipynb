{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to the Science Pipelines Software and Data products\n",
    "\n",
    "Now that you have brought yourself to the data via the Science Platform (Lesson 1), you can retrieve that data and rerun elements of the Science Pipelines. \n",
    "\n",
    "Today we'll cover:\n",
    "* What are the **Science Pipelines**?\n",
    "* What is this **Stack**?\n",
    "* What are the **data products**?\n",
    "* How to rerun an element of the pipelines, a **Task**, with a different configuration. \n",
    "\n",
    "We'll only quickly inspect the images and catalogs. Next week's lesson will be dedicated to learning more sophisticated methods for exploring the data.\n",
    "\n",
    "## 1. Overview of the Science Pipelines  and the stack (presentation)\n",
    "\n",
    "The stack is an implementation of the science pipelines and its corresponding library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  The Data Products\n",
    "\n",
    "Data products include both catalogs and images.\n",
    "\n",
    "Instead of operating directly on files and directories, we interact with on-disk data products via an abstraction layer called the data Butler. The butler operates on data repositories. DM regularly tests the science pipelines on precursor data from HSC, DECam, and simulated data generated by DESC we call DC2 ImSim. These DRP outputs can be found in `/datasets`. \n",
    "\n",
    "This notebook will be using HSC Gen 2 repo.\n",
    "\n",
    "*Jargon Watch: Gen2/Gen3 We're in the process of building a brand new Butler, which we are calling the 3rd Generation Butler,  or Gen3 for short.*\n",
    " \n",
    "| generation | Class |\n",
    "| ------------- | ------------- |\n",
    "|  Gen 2  | `lsst.daf.persistence.Butler`  |\n",
    "|  Gen 3   | `lsst.daf.butler.Butler` |\n",
    "\n",
    "A Gen 2 Butler is the first stack object we are going to instantiate, with a path to a directory that is a repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "REPO = '/datasets/hsc/repo/rerun/RC/w_2020_19/DM-24822'  \n",
    "from lsst.daf.persistence import Butler\n",
    "butler = Butler(REPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSC_REGISTRY_COLUMNS = ['taiObs', 'expId', 'pointing', 'dataType', 'visit', 'dateObs', 'frameId', 'filter', 'field', 'pa', 'expTime', 'ccdTemp', 'ccd', 'proposal', 'config', 'autoguider']\n",
    "butler.queryMetadata('calexp', HSC_REGISTRY_COLUMNS, dataId={'filter': 'HSC-I', 'visit': 30504, 'ccd': 50})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common error messages** when instantiating a Butler:\n",
    "\n",
    "1) \n",
    "```PermissionError: [Errno 13] Permission denied: '/datasets/hsc/repo/rerun/RC/w_2020_19/DM-248222'```\n",
    "- Translation: This directory does not exist. Confirm with  `os.path.exists(REPO)`\n",
    "\n",
    "2) `RuntimeError: No default mapper could be established from inputs`:\n",
    "\n",
    "- Translation: This directory exists, but is not a data repo. Does `REPO` have a file called `repositoryCfg.yaml` in it? Nope? It's not a data repo. Use `os.listdir` to see what's in your directory\n",
    "\n",
    "\n",
    "*Next we'll look at 3 types of data products:*\n",
    "* Images\n",
    "* Catalogs: lsst.afw.table\n",
    "* Catalogs: parquet/pyArrow DataFrames\n",
    "\n",
    "## 2.1 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VISIT = 34464\n",
    "CCD = 81\n",
    "exposure = butler.get('calexp', visit=int(VISIT), ccd=CCD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common error messages** when getting data:\n",
    "\n",
    "1) `'HscMapper' object has no attribute 'map_calExp'`\n",
    "- You're asking for a data product that doesn't exist. In this example, I asked for a 'calExp' with a capital E, which is not a thing.  Double check your spelling in: https://github.com/lsst/obs_base/blob/master/policy/exposures.yaml for images or  https://github.com/lsst/obs_base/blob/master/policy/datasets.yaml for catalogs or models.\n",
    "\n",
    "2) `NoResults: No locations for get: datasetType:calexp dataId:DataId(initialdata={'visit': 34464, 'ccd': 105}, tag=set())`:\n",
    "\n",
    "- This file doesn't exist. If you don't believe the Butler, add \"_filename\" to the data product you want, and you'll get back the filename you can lookup. For example:\n",
    "\n",
    "        butler.get('calexp_filename', visit=VISIT, ccd=105)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "butler.get('calexp_filename', visit=VISIT, ccd=105)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rare error message: Did you try that and now it says it can't find the filename? `NoResults: No locations for get: datasetType:calexp_filename dataId:DataId(initialdata={'visit': 34464, 'ccd': 81}, tag=set())` Sqlalchemy doesn't handle data types well. Force your visit or ccd numbers to be integers like `butler.get('calexp', visit=int(34464), ...`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: If I can get the filename from the butler, why can't I just read it in manually like I do other fits files and fits tables?\n",
    "\n",
    "A: Because in operations, the data will not be on a shared GPFS disk like you're reading from now. We guarantee `butler.get` to work the same regardless of the backend storage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exposure Objects\n",
    "\n",
    "The data that the butler just fetched for us is an `Exposure` object. It composes a `maskedImage` which has 3 `Image` object: an `image`, `mask`, and `variance`.  These are pointers/views!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exposure.maskedImage.image   \n",
    "exposure.maskedImage.mask\n",
    "exposure.maskedImage.variance\n",
    "\n",
    "# These shortcuts work too.\n",
    "exposure.image\n",
    "exposure.variance\n",
    "exposure.mask\n",
    "\n",
    "# each image also has an array property e.g.\n",
    "exposure.image.array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exposures also include a WCS Object, a PSF Object and ExposureInfo. These can be accessed via the following methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcs = exposure.getWcs()\n",
    "psf = exposure.getPsf()\n",
    "photoCalib = exposure.getPhotoCalib()\n",
    "expInfo = exposure.getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitInfo = expInfo.getVisitInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Use tab-complete or '?exposure' to explore these Exposure.  Explore details are in this visit info. What was the exposure time? What was the observation date? Exploring the other methods of Exposure object, what are the dimensions of the image? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visitInfo.\n",
    "# exposure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more documentation on Exposure objects:\n",
    "* https://pipelines.lsst.io/modules/lsst.afw.image/indexing-conventions.html\n",
    "\n",
    "For another notebook on Exposure objects:\n",
    "* https://github.com/LSSTScienceCollaborations/StackClub/blob/master/Basics/Calexp_guided_tour.ipynb\n",
    "\n",
    "Session 3 will introduce more sophisticated image display tools, and go into detail on the `Display` objects in the stack, but let's take a quick look at this image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import lsst.afw.display as afw_display\n",
    "%matplotlib inline\n",
    "\n",
    "matplotlib.rcParams[\"figure.figsize\"] = (6, 4)\n",
    "matplotlib.rcParams[\"font.size\"] = 12\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's smooth it first just for fun. \n",
    "\n",
    "from skimage.filters import gaussian\n",
    "exposure.image.array[:] = gaussian(exposure.image.array, sigma=5)\n",
    "\n",
    "# and display it\n",
    "display = afw_display.Display(frame=1, backend='matplotlib')\n",
    "display.scale(\"linear\", \"zscale\")\n",
    "display.mtv(exposure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the colorbar, you can tell that the background has been subtracted.\n",
    "\n",
    "The first step of the pipeline, `processCcd` takes a `postISRCCD` as input. Before detection and measurement, it estimates a naive background and subtracts it.  Indeed we store a `calexp` with the background subtracted and model that was subtracted from the original `postISRCCD` as the `calexpBackground`.\n",
    "\n",
    "There's full focal plane background estimation step that produces a delta on the `calexpBackground`, which is called `skyCorr`. \n",
    "\n",
    "Let's quickly plot these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch background models from the butler\n",
    "background = butler.get('calexpBackground', visit=VISIT, ccd=CCD)\n",
    "skyCorr = butler.get('skyCorr', visit=VISIT, ccd=CCD)\n",
    "\n",
    "# call \"getImage\" to evaluate the model on a pixel grid\n",
    "plt.subplot(121)\n",
    "plt.imshow(background.getImage().array, origin='lower', cmap='gray')\n",
    "plt.title(\"Local Polynomial Bkgd\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(background.getImage().array - skyCorr.getImage().array, origin='lower', cmap='gray')\n",
    "plt.title(\"SkyCorr Bkgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exposure = butler.get('calexp', visit=VISIT, ccd=CCD)\n",
    "background = butler.get('calexpBackground', visit=VISIT, ccd=CCD)\n",
    "# create a view to the masked image\n",
    "mi = exposure.maskedImage\n",
    "# add the background image to that view.\n",
    "mi += background.getImage()\n",
    "\n",
    "display1 = afw_display.Display(frame=1, backend='matplotlib')\n",
    "display1.scale(\"linear\", \"zscale\")\n",
    "exposure.image.array[:] = gaussian(exposure.image.array, sigma=5)\n",
    "display1.mtv(exposure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good to get in the habit of mathmatical operations on maskedImages instead of images, because it scales the variance plane appropriately. For example, when you multiply a `MaskedImage` by 2, it multiplies the `Image` by 2 and the `Variance` by 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.1)** Coadds have dataId's defined by their SkyMap. Fetch the `deepCoadd` with `tract=9813`, `patch='3,3'` and `filter='HSC-I'` from the same repo. \n",
    "\n",
    "Bonus: a `deepCoadd_calexp` has had an additional aggressive background model applied called a `deepCoadd_calexp_background`. Confirm that the `deepCoadd_calexp` + `deepCoadd_calexp_background` = `deepCoadd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepCoadd = butler.get('deepCoadd', tract=9813, patch='3,3', filter='HSC-I')\n",
    "deepCoadd_calexp_background = butler.get('deepCoadd_calexp_background', tract=9813, patch='3,3', filter='HSC-I')\n",
    "deepCoadd_calexp = butler.get('deepCoadd_calexp', tract=9813, patch='3,3', filter='HSC-I')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = deepCoadd_calexp.maskedImage\n",
    "mi += deepCoadd_calexp_background.getImage()\n",
    "\n",
    "deepCoadd.image.array - deepCoadd_calexp.image.array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Catalogs (lsst.afw.table format)\n",
    "\n",
    "\n",
    "afwTables are for passing to tasks.  The pipeline needed C++ readable table format, so we wrote one. If you want to pass a catalog to a Task, it'll probably take one of these. They are:\n",
    "* Row stores and\n",
    "* the column names are oriented for software\n",
    "\n",
    "The source table immediatly output by processCcd is called `src`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = butler.get('src', visit=VISIT, ccd=CCD)\n",
    "src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned object, `src`, is a `lsst.afw.table.SourceCatalog` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src.getSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the schema reveals that instFluxes are in uncalibrated units of counts. coord_ra/coord_dec are in units of radians. \n",
    "\n",
    "`lsst.afw.table.SourceCatalog`s have their own API. However if you are *just* going to use it for analysis, you can convert it to an AstroPy table or a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src.asAstropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = src.asAstropy().to_pandas()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Catalogs (Parquet/PyArrow DataFrame format)\n",
    "\n",
    "* Output data product ready for analysis\n",
    "* Column store\n",
    "* Full visit and full tract options\n",
    "* Column names\n",
    "\n",
    "The parquet outputs have been transformed to database-specified units. Fluxes are in nanojanskys, coordinates are in degrees. These will match what you get via the Portal and the Catalog Access tool Simon showed last week.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parq = butler.get('sourceTable', visit=VISIT, ccd=CCD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ParquetTable` is just a light  wrapper around a `pyarrow.parquet.ParquetFile`.\n",
    "\n",
    "You can get a parquet table from the butler, but it doesn't fetch any columns until you ask for them. It's a column store which means that it can read only one column at a time. This is great for analysis when you want to plot two million element arrays. In a row-store you'd have to read the whole ten million-row table just for those two columns you wanted. \n",
    "\n",
    "But don't even try to loop through rows! If you want a whole row, use the `afwTable`. \n",
    "\n",
    "Last I checked the processing step that consolidates the Source Tables from a per-ccd Source Table to a \n",
    "\n",
    "`parq = butler.get('sourceTable', visit=VISIT)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the columns with: \n",
    "parq.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the column names are different. Now fetch just the columns you want. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parq.toDataFrame(columns=['ra', 'decl', 'PsFlux', 'PsFluxErr', 'sky_source',\n",
    "                               'PixelFlags_bad', 'PixelFlags_sat', 'PixelFlags_saturated'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Using this DataFrame `df`, make a histogram of `PsFlux` for sky sources using this parquet source table. If `sky_source == True` then the source was not a normal detection, but rather an randomly placed centroid to measure properties of blank sky. The distribution should be centered at 0. \n",
    "\n",
    "\n",
    "**Exercise:** A parquet `objectTable_tract` contains deep coadd measurements for 1.5 sq. deg. tract.\n",
    "**Make a r-i vs. g-r plot** of stars with a r-band SNR > 100. Use `refExtendedness` == 0 to select for stars. It means that the galaxy model Flux was similar to the PSF Flux. By the looks of your plot, what do you think about using refExtendedness for star galaxy separation?*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# butler = Butler('/datasets/hsc/repo/rerun/DM-23243/OBJECT/DEEP')\n",
    "# parq = butler.get('objectTable_tract', tract=9813)\n",
    "# parq.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "\n",
    "\n",
    "**TL;DR  If you remember one thing about tasks it's go to http://pipelines.lsst.io, then click on lsst.pipe.base**\n",
    "\n",
    "On the landing page for lsst.pipe.base documenation https://pipelines.lsst.io/modules/lsst.pipe.base/index.html, you'll see a number of tutorials on how to use Tasks and how to create one.\n",
    "\n",
    "CmdlineTask extends Task with commandline driver utils for use with Gen2 Butlers, and will be deprecated soon. However, not all the links under \"CommandlineTask\" will become obsolete. For example, Retargeting subtasks of command-line tasks will live on.\n",
    "\n",
    "Read: https://pipelines.lsst.io/modules/lsst.pipe.base/task-framework-overview.html\n",
    "\n",
    "What is a Task?\n",
    "Tasks implement astronomical data processing functionality. They are:\n",
    "\n",
    "* **Configurable:** Modify a task’s behavior by changing its configuration. Automatically apply camera-specific modifications\n",
    "* **Hierarchical:** Tasks can call other tasks as subtasks\n",
    "* **Extensible:** Replace (“retarget”) any subtask with a variant. Write your own subclass of a task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edited highlights of ${PIPE_TASKS_DIR}/example/exampleStatsTask.py\n",
    "import sys\n",
    "import numpy as np\n",
    "from lsst.geom import Box2I, Point2I, Extent2I\n",
    "from lsst.afw.image import MaskedImageF\n",
    "from lsst.pipe.tasks.exampleStatsTasks import ExampleSimpleStatsTask, ExampleSigmaClippedStatsTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a MaskedImageF -- an image containing floats\n",
    "# together with a mask and a per-pixel variance.\n",
    "\n",
    "WIDTH = 40\n",
    "HEIGHT = 20\n",
    "\n",
    "maskedImage = MaskedImageF(Box2I(Point2I(10, 20),\n",
    "                                 Extent2I(WIDTH, HEIGHT)))\n",
    "x = np.random.normal(10, 20, size=WIDTH*HEIGHT)\n",
    "\n",
    "# Because we are shoving it into an ImageF and numpy defaults\n",
    "# to double precision\n",
    "X = x.reshape(HEIGHT, WIDTH).astype(np.float32)  \n",
    "im = maskedImage.image\n",
    "im.array = X\n",
    "\n",
    "# We initialize the Task once but can call it many times.\n",
    "task = ExampleSimpleStatsTask()\n",
    "\n",
    "# Simply call the .run() method with the MaskedImageF.\n",
    "# Most Tasks have a .run() method. Look there first. \n",
    "result = task.run(maskedImage)\n",
    "\n",
    "# And print the result.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Task with configuration\n",
    "\n",
    "Now we are going to instantiate Tasks with two different configs. Configs must be set *before* instantiating the task. Do not change the config of an already-instatiated Task object. It will not do what you think it's doing. \n",
    "\n",
    "In fact, during commandline processing, the `Task` drivers such as `CmdLineTask` freeze the configs before running the Task. When you're running them from notebooks, they are not frozen, hence this warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edited highlights of ${PIPE_TASKS_DIR}/example/exampleStatsTask.py\n",
    "\n",
    "config1 = ExampleSigmaClippedStatsTask.ConfigClass(numSigmaClip=1)\n",
    "\n",
    "config2 = ExampleSigmaClippedStatsTask.ConfigClass()\n",
    "config2.numSigmaClip = 3\n",
    "\n",
    "task1 = ExampleSigmaClippedStatsTask(config=config1)\n",
    "task2 = ExampleSigmaClippedStatsTask(config=config2)\n",
    "\n",
    "print(task1.run(maskedImage).mean)\n",
    "print(task2.run(maskedImage).mean)\n",
    "\n",
    "\n",
    "# Example of what not to do\n",
    "# -------------------------\n",
    "# task1 = ExampleSigmaClippedStatsTask(config=config1)\n",
    "# print(task1.run(maskedImage).mean)\n",
    "# DO NOT EVER DO THIS!\n",
    "# task1.config.numSigmaClip = 3  <--- bad bad bad \n",
    "# print(task1.run(maskedImage).mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Subtraction and Task Configuration\n",
    "\n",
    "The following example of reconfiguring a task is one step in an introduction to `processCcd`: https://github.com/lsst-sqre/notebook-demo/blob/master/AAS_2019_tutorial/intro-process-ccd.ipynb\n",
    "\n",
    "`processCcd`, our basic source extractor run as the first step step in the pipeline, will be covered in more detail in Session 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lsst.meas.algorithms import SubtractBackgroundTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the background back in so that we can remodel it (like we did above)\n",
    "\n",
    "postISRCCD  = butler.get(\"calexp\", visit=30502, ccd=CCD)\n",
    "bkgd = butler.get(\"calexpBackground\", visit=30502, ccd=CCD)\n",
    "mi = exposure.maskedImage\n",
    "mi += bkgd.getImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to get fun & terrible results!\n",
    "bkgConfig = SubtractBackgroundTask.ConfigClass()\n",
    "bkgConfig.useApprox = False\n",
    "bkgConfig.binSize = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The `config` object here is an instance of a class that inherits from `lsst.pex.config.Config` that contains a set of `lsst.pex.config.Field` objects that define the options that can be modified.  Each `Field` behaves more or less like a Python `property`, and you can get information on all of the fields in a config object by either using `help`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(bkgConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SubtractBackgroundTask.ConfigClass.algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkgTask = SubtractBackgroundTask(config=bkgConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkgResult = bkgTask.run(exposure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display1 = afw_display.Display(frame=1, backend='matplotlib')\n",
    "display1.scale(\"linear\", min=-0.5, max=10)\n",
    "display1.mtv(exposure[700:1400,1800:2400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've run through all of these steps after executing the cell that warns about terrible results, you should notice that the galaxy in the upper right has been oversubtracted.\n",
    "\n",
    "**Exercise**: Before continuing on, re-load the exposure from disk, reset the configuration and `Task` instances, and re-run without executing the cell that applies bad values to the config, all by just re-executing the right cells above.  You should end up an image in which the upper-right galaxy looks essentially the same as it does in the image before we subtracted the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
