{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulge data exploration\n",
    "\n",
    "In Part A, we showed how to ingest raw DECam images and to process them. A processed data set is available at `/project/stack-club/course_data/DECAM_BULGE`. In this notebook we will use the Butler to explore the processed data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plots available to the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.table import Table\n",
    "from astropy.visualization import hist\n",
    "import matplotlib.pyplot as plt\n",
    "import lsst.daf.persistence as dafPersist\n",
    "import lsst.afw.table as afwTable\n",
    "import lsst.afw.display as afwDisplay\n",
    "\n",
    "afwDisplay.setDefaultBackend('matplotlib') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repo directory and rerun directory\n",
    "REPO_DIR = '/project/stack-club/course_data/DECAM_BULGE/' \n",
    "RERUN_DIR = REPO_DIR + \"rerun\"\n",
    "\n",
    "# Directory where we save our data, like pandas data frames, tables, light curves ...\n",
    "parquet_save_path = '/home/mrabus/DATA/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After processing the images we can start with the butler and displaying images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Butler with the rerun directory of the processed DECam Bulge data \n",
    "butler = dafPersist.Butler(RERUN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We create a two pandas data frames and a list of cal. exposures for CCD1.\n",
    "\n",
    "We have go through the metadata and validate for each dataId that it exits. If the dataset exits we write the coordinates min., max., and center based on the source catalog into a pandas data frame. All CCDs for a certain pointing should have the same time, i.e. you point all CCDs at once to the field, therefore, we only need one data frame which associates the visit to the time of observation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate_list = []\n",
    "calexp_list = []\n",
    "visit_date_list = []\n",
    "metadata = butler.queryMetadata('calexp',['visit','ccd','filter'])\n",
    "# Iterate to metadata\n",
    "for dataset in metadata:\n",
    "    dataId = {'visit': int(dataset[0]), 'ccd': int(dataset[1]), 'filter':dataset[2]}\n",
    "    #Check if the data set has a source catalog\n",
    "    if butler.datasetExists('src', dataId=dataId):\n",
    "        srcCatalog = butler.get('src', dataId=dataId).asAstropy() # get the source catalog\n",
    "        # get the minimum and maximum RA/DEC\n",
    "        raMax = srcCatalog['coord_ra'].max()\n",
    "        raMin = srcCatalog['coord_ra'].min()\n",
    "        decMax = srcCatalog['coord_dec'].max()\n",
    "        decMin = srcCatalog['coord_dec'].min()\n",
    "        #Calculate the center \n",
    "        raCenter = 0.5*(raMax + raMin)\n",
    "        decCenter = 0.5*(decMax + decMin)\n",
    "        # Number of detected sources\n",
    "        nr_detected_sources = len(srcCatalog)\n",
    "        # Get the median effective PSF area\n",
    "        medianPSFarea = np.median(srcCatalog['base_PsfFlux_area'])\n",
    "        #Append to the list.\n",
    "        coordinate_list.append([int(dataset[0]), int(dataset[1]), dataset[2], raCenter, decCenter, \n",
    "                  raMin, raMax, decMin, decMax, nr_detected_sources, medianPSFarea])\n",
    "        # for ccd1, create a list with visit and time of observation. (Should be the same for all other CCDs)\n",
    "        if int(dataset[1]) == 1:\n",
    "            # get the calexp for CCD1\n",
    "            calexp = butler.get('calexp', visit=int(dataset[0]), ccd=1)\n",
    "            # Append the calexp in the list\n",
    "            calexp_list.append( calexp )\n",
    "            #Get visit info, to extract time of observation\n",
    "            exp_visit_info = calexp.getInfo().getVisitInfo()\n",
    "            visit_date = exp_visit_info.getDate()\n",
    "            visit_date_list.append( [int(dataset[0]), visit_date.toPython()] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the lists to a panda data frame \n",
    "df_valid_visists = pd.DataFrame(coordinate_list, columns=['visit', 'ccd', 'DECAM_filter', 'ra_center', 'dec_center', \n",
    "                                        'ra_min', 'ra_max', 'dec_min', 'dec_max', 'nr_detected_sources', 'median_effPSF_area'])\n",
    "df_visit_date = pd.DataFrame(visit_date_list, columns=['visit', 'timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the pandas data frame as parque on disk\n",
    "df_valid_visists.to_parquet( os.path.join(parquet_save_path,'df_valid_visits.parquet.gzip'), compression='gzip')\n",
    "df_visit_date.to_parquet( os.path.join(parquet_save_path,'df_visits_date.parquet.gzip'), compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the paque files\n",
    "df_valid_visists = pd.read_parquet( os.path.join(parquet_save_path,'df_valid_visits.parquet.gzip'), engine='fastparquet' )\n",
    "df_visit_data = pd.read_parquet( os.path.join(parquet_save_path,'df_visits_date.parquet.gzip'), engine='fastparquet' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the visit time of observations\n",
    "df_visit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the beginning of the data frame which has all valid visits and coordinates in it.\n",
    "df_valid_visists.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get unique visit IDs\n",
    "df_valid_visists.visit.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query all ccd 1 in the pandas data frame\n",
    "valid_visit_ccd1 = df_valid_visists.query('ccd == 1')\n",
    "valid_visit_ccd1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all pointing centers for CCD 1 are in the same field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the standard deviation of the pointing centers for each visit and for CCD1 in arcsec\n",
    "print(valid_visit_ccd1['ra_center'].std()*u.deg.to(u.arcsec),valid_visit_ccd1['dec_center'].std()*u.deg.to(u.arcsec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the first ten images of CCD1 to inspect visually the pointing:\n",
    "\n",
    "for ii,calexp in enumerate(calexp_list[:10]):\n",
    "\n",
    "    plt.figure(ii)\n",
    "    display = afwDisplay.Display(frame=ii, backend='matplotlib')\n",
    "    display.scale(\"linear\", \"zscale\")\n",
    "    #display only a small region of the calexp.\n",
    "    display.mtv(calexp[500:1500,2500:3000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort valid visit from ccd1 \n",
    "valid_visit_ccd1 = valid_visit_ccd1.sort_values(by=['nr_detected_sources'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_visit_ccd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_visits = valid_visit_ccd1.visit.unique()\n",
    "ccd = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataId={'visit': int(unique_visits[0]), 'ccd': ccd}\n",
    "srcCatalog1 = butler.get('src', dataId=dataId).asAstropy().to_pandas()\n",
    "dataId={'visit': int(unique_visits[1]), 'ccd': ccd}\n",
    "srcCatalog2 = butler.get('src', dataId=dataId).asAstropy().to_pandas()\n",
    "\n",
    "srcCatalog1 = srcCatalog1.sort_values(by=['id'])\n",
    "srcCatalog2 = srcCatalog2.sort_values(by=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcCatalog1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcCatalog2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dist = np.array([])\n",
    "#create master star list\n",
    "\n",
    "\n",
    "dataId={'visit': int(unique_visits[0]), 'ccd': ccd}\n",
    "\n",
    "srcCatalog = butler.get('src', dataId=dataId) # get the source catalog for the first visit and make this the master star list\n",
    "master_coordinates = SkyCoord(srcCatalog['coord_ra']*u.deg, srcCatalog['coord_dec']*u.deg)\n",
    "master_starID = srcCatalog['id']\n",
    "\n",
    "coord_table = Table([master_starID, master_coordinates], names=('id', int(unique_visits[0])))\n",
    "\n",
    "for visit in unique_visits[1:]:\n",
    "    dataId={'visit': int(visit), 'ccd': ccd}\n",
    "    srcCatalog = butler.get('src', dataId=dataId)\n",
    "    coordinates = SkyCoord(srcCatalog['coord_ra']*u.deg, srcCatalog['coord_dec']*u.deg)\n",
    "    idx, d2d, d3d = master_coordinates.match_to_catalog_sky(coordinates)\n",
    "    coord_table.add_column(coordinates[idx], name=int(visit))\n",
    "    coord_table[f'distance {int(visit)}'] = d2d.arcsec*u.arcsec\n",
    "    all_dist = np.append(all_dist,d2d.arcsec)\n",
    "    print('visit: {}       max. dist. {:.3f} arcsec       std. dist. {:.3f} arcsec'.format(visit, np.max(d2d.arcsec), np.std(d2d.arcsec)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of distances\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "hist(all_dist, bins='freedman', ax=ax, histtype='stepfilled',\n",
    "         alpha=0.75, density=True)\n",
    "ax.set_xlabel('distance [arcsec]')\n",
    "ax.set_ylabel('Density(distance)')\n",
    "ax.set_xlim(-0.001,0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "- Check astrometry\n",
    "- Make light curves\n",
    "- create co-add image\n",
    "- run ap-pipe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
